{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anlay_ridi.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPNLaMVncml0JVEoIk1b2Iq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/world970511/RIDIBOOKS_romance_webnovel_review_Sentiment_Analysis/blob/main/anlay_ridi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udSZRsShTN3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "903bae6b-05a1-4cae-8871-29879b444124"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import urllib.request\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/world970511/RIDIBOOKS_romance_webnovel_review_Sentiment_Analysis/main/data/result_all_data.txt\",\n",
        "                        filename=\"result_all_data.txt\")\n",
        "all_data = pd.read_csv('result_all_data.txt',encoding='utf-8')\n",
        "print(all_data)\n",
        "\n",
        "print('\\n===================\\n')\n",
        "print('초반 데이터 확인 :',len(all_data))\n",
        "all_data.drop_duplicates(subset=['review'], inplace=True)#중복된 리뷰들을 제거한다\n",
        "print('중복 제거 확인 :',len(all_data))\n",
        "\n",
        "all_data = all_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "print('null값이 존재하는가?=',all_data.isnull().values.any()) # Null 값이 존재하는지 확인\n",
        "print('\\n===================\\n')\n",
        "\n",
        "train_data, test_data = train_test_split(all_data, test_size = 0.25, random_state = 42)#3:1로 데이터 분할\n",
        "print('훈련용 리뷰의 개수 :', len(train_data))\n",
        "print('테스트용 리뷰의 개수 :', len(test_data))\n",
        "\n",
        "print('\\n===================\\n')\n",
        "train_data.groupby('rate').size().reset_index(name = 'count')\n",
        "test_data.groupby('rate').size().reset_index(name = 'count')\n",
        "\n",
        "train_data=train_data.fillna(\" \")#훈련 데이터에서 한글과 공백을 제외하고 제거\n",
        "train_data['review']=train_data['review'].apply(lambda x:re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\",x))\n",
        "train_data['review']=train_data['review'].apply(lambda x:re.sub('^ +',\"\",x))\n",
        "train_data['review'].replace('', np.nan, inplace=True)\n",
        "train_data = train_data.dropna(how='any')\n",
        "print('전처리 후 훈련용 샘플의 개수 :',len(train_data))\n",
        "\n",
        "test_data=test_data.fillna(\" \")#테스트 데이터에서 한글과 공백을 제외하고 제거\n",
        "test_data['review']=test_data['review'].apply(lambda x:re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\",x))\n",
        "test_data['review']=test_data['review'].apply(lambda x:re.sub('^ +',\"\",x))\n",
        "test_data['review'].replace('', np.nan, inplace=True)\n",
        "test_data = test_data.dropna(how='any')\n",
        "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))\n",
        "print('\\n===================\\n')\n",
        "\n",
        "test_data.to_csv('test.txt',mode='w',index=False)\n",
        "train_data.to_csv('train.txt',mode='w',index=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        rate                                             review\n",
            "0          1  믿고 보는 작가님 짱  남주 여주 넘 좋아요  여주때문에 안달 못하는 모습도 재미있...\n",
            "1          1                                        기대됩니다 잘읽을께용\n",
            "2          0                                하아ㅜㅜ 진짜 남주가 로봇같아요ㅜㅜ\n",
            "3          0   필력이 진짜... 없어요 이런 판타지 로맨스를 좋아해서 기대하면서 봤는데 휴......\n",
            "4          0         갈수록 지루하고 유치해져요.  길지도 않은 책인데 끝까지 못읽고 포기했어요.\n",
            "...      ...                                                ...\n",
            "110863     1            재밋게 잘봣어요.ㅎ너무빨리읽어져서아쉽네요 작가님다른작품도 찾아봐야겟어요\n",
            "110864     0  .개취라지만 납득안감..여주는 한장넘어갈때마다 울고 눈물흘리고..혼자 걷지도 못하는...\n",
            "110865     1                              좋아하는 작가님 신작입니다! 기대돼요~\n",
            "110866     1                                작가님 책 다 봤어요. 잼있어요~~\n",
            "110867     0  ㅋㅋㅋ제목 잘못된수준이에요ㅋㅋ 꽃잎은 젖어든다가 아니라 젖어있다라던가 마를날이 없다...\n",
            "\n",
            "[110868 rows x 2 columns]\n",
            "\n",
            "===================\n",
            "\n",
            "초반 데이터 확인 : 110868\n",
            "중복 제거 확인 : 110868\n",
            "null값이 존재하는가?= False\n",
            "\n",
            "===================\n",
            "\n",
            "훈련용 리뷰의 개수 : 83151\n",
            "테스트용 리뷰의 개수 : 27717\n",
            "\n",
            "===================\n",
            "\n",
            "전처리 후 훈련용 샘플의 개수 : 83100\n",
            "전처리 후 테스트용 샘플의 개수 : 27697\n",
            "\n",
            "===================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9j-w6FvKRCX",
        "outputId": "f9ac8392-4fb8-4a0f-8c1c-6adf49bfc62c"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 38.5MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.7MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, beautifulsoup4, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkmq-_80J2PJ",
        "outputId": "889d0cc2-a8f6-4c02-9799-21e4258cbb2b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if(len(s) <= max_len):\n",
        "        cnt = cnt + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\n",
        "\n",
        "\n",
        "train_data=pd.read_csv('train.txt',encoding='utf-8')\n",
        "test_data=pd.read_csv('test.txt',encoding='utf-8')\n",
        "\n",
        "okt = Okt()\n",
        "stopwords = ['의','가','이','은','들','는','좀','원','권','과','도','를','으로','자','에','와','한','하다','뽕빨','제','금']\n",
        "\n",
        "X_train = []\n",
        "for sentence in train_data['review']:\n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "    X_train.append(temp_X)\n",
        "\n",
        "X_test = []\n",
        "for sentence in test_data['review']:\n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "    X_test.append(temp_X)\n",
        "\n",
        "print('\\n===================\\n')\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
        "\n",
        "vocab_size = total_cnt - rare_cnt + 1\n",
        "print('단어 집합의 크기 :',vocab_size)\n",
        "\n",
        "tokenizer = Tokenizer(vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "y_train = np.array(train_data['rate'])\n",
        "y_test = np.array(test_data['rate'])\n",
        "\n",
        "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
        "\n",
        "# 빈 샘플들을 제거\n",
        "X_train = np.delete(X_train, drop_train, axis=0)\n",
        "y_train = np.delete(y_train, drop_train, axis=0)\n",
        "print(len(X_train))\n",
        "\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
        "\n",
        "max_len = 85\n",
        "below_threshold_len(max_len, X_train)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen = max_len)\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "===================\n",
            "\n",
            "단어 집합(vocabulary)의 크기 : 32208\n",
            "등장 빈도가 2번 이하인 희귀 단어의 수: 17419\n",
            "단어 집합에서 희귀 단어의 비율: 54.08283656234476\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.25971812003049\n",
            "단어 집합의 크기 : 14790\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "83072\n",
            "리뷰의 최대 길이 : 725\n",
            "리뷰의 평균 길이 : 20.177881837442218\n",
            "전체 샘플 중 길이가 85 이하인 샘플의 비율: 97.3432684899846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zFLZ7W8MZe3",
        "outputId": "17fcf545-068a-43df-bd18-eafdd720345e"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n",
        "\n",
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1108/1108 [==============================] - 18s 15ms/step - loss: 0.2947 - acc: 0.8815 - val_loss: 0.2642 - val_acc: 0.8955\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.89552, saving model to best_model.h5\n",
            "Epoch 2/15\n",
            "1108/1108 [==============================] - 15s 14ms/step - loss: 0.2313 - acc: 0.9109 - val_loss: 0.2428 - val_acc: 0.9056\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.89552 to 0.90563, saving model to best_model.h5\n",
            "Epoch 3/15\n",
            "1108/1108 [==============================] - 14s 13ms/step - loss: 0.2093 - acc: 0.9204 - val_loss: 0.2343 - val_acc: 0.9080\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.90563 to 0.90803, saving model to best_model.h5\n",
            "Epoch 4/15\n",
            "1108/1108 [==============================] - 14s 13ms/step - loss: 0.1932 - acc: 0.9282 - val_loss: 0.2339 - val_acc: 0.9112\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.90803 to 0.91122, saving model to best_model.h5\n",
            "Epoch 5/15\n",
            "1108/1108 [==============================] - 14s 13ms/step - loss: 0.1806 - acc: 0.9345 - val_loss: 0.2249 - val_acc: 0.9160\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.91122 to 0.91604, saving model to best_model.h5\n",
            "Epoch 6/15\n",
            "1108/1108 [==============================] - 14s 13ms/step - loss: 0.1694 - acc: 0.9394 - val_loss: 0.2350 - val_acc: 0.9147\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.91604\n",
            "Epoch 7/15\n",
            "1108/1108 [==============================] - 14s 13ms/step - loss: 0.1594 - acc: 0.9431 - val_loss: 0.2360 - val_acc: 0.9106\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.91604\n",
            "Epoch 8/15\n",
            "1108/1108 [==============================] - 14s 13ms/step - loss: 0.1497 - acc: 0.9469 - val_loss: 0.2317 - val_acc: 0.9125\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.91604\n",
            "Epoch 9/15\n",
            "1108/1108 [==============================] - 14s 13ms/step - loss: 0.1410 - acc: 0.9509 - val_loss: 0.2343 - val_acc: 0.9110\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.91604\n",
            "Epoch 00009: early stopping\n",
            "866/866 [==============================] - 4s 4ms/step - loss: 0.2239 - acc: 0.9164\n",
            "\n",
            " 테스트 정확도: 0.9164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9-v1UwxMYxm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lIXQYR4NtZM",
        "outputId": "71ae04a3-a479-404a-ede0-0937d5fe3f00"
      },
      "source": [
        "def sentiment_predict(new_sentence):\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
        "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
        "  score = float(loaded_model.predict(pad_new)) # 예측\n",
        "  if(score > 0.5):\n",
        "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
        "  else:\n",
        "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))\n",
        "\n",
        "  \n",
        "if __name__ == \"__main__\": \n",
        "  while(True):\n",
        "    new_sentence=input(\"리뷰 테스트(종료하고 싶다면 n을 쳐주세요):\")\n",
        "    if  new_sentence !='n':\n",
        "      try:\n",
        "        sentiment_predict(new_sentence) \n",
        "      except:\n",
        "        break\n",
        "        print('stop')\n",
        "    else: break"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "리뷰 테스트(종료하고 싶다면 n을 쳐주세요):작가님 외전 주세요 제빌\n",
            "96.98% 확률로 긍정 리뷰입니다.\n",
            "\n",
            "리뷰 테스트(종료하고 싶다면 n을 쳐주세요):가독성이 너무 떨어져요\n",
            "99.11% 확률로 부정 리뷰입니다.\n",
            "\n",
            "리뷰 테스트(종료하고 싶다면 n을 쳐주세요):재밌게 읽었습니다\n",
            "97.95% 확률로 긍정 리뷰입니다.\n",
            "\n",
            "리뷰 테스트(종료하고 싶다면 n을 쳐주세요):설정도 좋고 필력도 좋으신데 뒤로 가면 갈수록 이야기가 이상해지네요\n",
            "84.61% 확률로 부정 리뷰입니다.\n",
            "\n",
            "리뷰 테스트(종료하고 싶다면 n을 쳐주세요):스토리라인이 이상해요;; 돈아깝네요\n",
            "99.66% 확률로 부정 리뷰입니다.\n",
            "\n",
            "리뷰 테스트(종료하고 싶다면 n을 쳐주세요):여주고 남주고 둘다 이상한거 같은데\n",
            "96.08% 확률로 부정 리뷰입니다.\n",
            "\n",
            "리뷰 테스트(종료하고 싶다면 n을 쳐주세요):재밌어요. 대여해놓고  왠지 미루다미루다 봤는데..왠걸 재밌어요. 근데 댓글들처럼 복수가아숩네요.\n",
            "97.96% 확률로 긍정 리뷰입니다.\n",
            "\n",
            "리뷰 테스트(종료하고 싶다면 n을 쳐주세요):기대치가 너무 컸나봐요\n",
            "80.20% 확률로 부정 리뷰입니다.\n",
            "\n",
            "리뷰 테스트(종료하고 싶다면 n을 쳐주세요):n\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}