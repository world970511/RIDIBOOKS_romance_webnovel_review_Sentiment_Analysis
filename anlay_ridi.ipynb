{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anlay_ridi.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVY7vLFAEWiB+HHSCkYx1j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/world970511/RIDIBOOKS_romance_webnovel_review_Sentiment_Analysis/blob/main/anlay_ridi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udSZRsShTN3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9fa4026-bb4e-4cd8-87c1-fc89b61dd587"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import urllib.request\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/world970511/RIDIBOOKS_romance_webnovel_review_Sentiment_Analysis/main/data/result_all_data.txt\",\n",
        "                        filename=\"result_all_data.txt\")\n",
        "all_data = pd.read_csv('result_all_data.txt',encoding='utf-8')\n",
        "print(all_data)\n",
        "\n",
        "print('\\n===================\\n')\n",
        "print('초반 데이터 확인 :',len(all_data))\n",
        "all_data.drop_duplicates(subset=['review'], inplace=True)#중복된 리뷰들을 제거한다\n",
        "print('중복 제거 확인 :',len(all_data))\n",
        "\n",
        "all_data = all_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "print('null값이 존재하는가?=',all_data.isnull().values.any()) # Null 값이 존재하는지 확인\n",
        "print('\\n===================\\n')\n",
        "\n",
        "train_data, test_data = train_test_split(all_data, test_size = 0.25, random_state = 42)#3:1로 데이터 분할\n",
        "print('훈련용 리뷰의 개수 :', len(train_data))\n",
        "print('테스트용 리뷰의 개수 :', len(test_data))\n",
        "\n",
        "print('\\n===================\\n')\n",
        "train_data.groupby('rate').size().reset_index(name = 'count')\n",
        "test_data.groupby('rate').size().reset_index(name = 'count')\n",
        "\n",
        "train_data=train_data.fillna(\" \")#훈련 데이터에서 한글과 공백을 제외하고 제거\n",
        "train_data['review']=train_data['review'].apply(lambda x:re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\",x))\n",
        "train_data['review']=train_data['review'].apply(lambda x:re.sub('^ +',\"\",x))\n",
        "train_data['review'].replace('', np.nan, inplace=True)\n",
        "train_data = train_data.dropna(how='any')\n",
        "print('전처리 후 훈련용 샘플의 개수 :',len(train_data))\n",
        "\n",
        "test_data=test_data.fillna(\" \")#테스트 데이터에서 한글과 공백을 제외하고 제거\n",
        "test_data['review']=test_data['review'].apply(lambda x:re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\",x))\n",
        "test_data['review']=test_data['review'].apply(lambda x:re.sub('^ +',\"\",x))\n",
        "test_data['review'].replace('', np.nan, inplace=True)\n",
        "test_data = test_data.dropna(how='any')\n",
        "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))\n",
        "print('\\n===================\\n')\n",
        "\n",
        "test_data.to_csv('test.txt',mode='w',index=False)\n",
        "train_data.to_csv('train.txt',mode='w',index=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       rate                                             review\n",
            "0         1                                        사스가 내 취향...\n",
            "1         1  재밌어요~ 근데 재탕은 조금 힘드네요.. 처음 읽을 때도 중간에 늘어지는 부분 있는...\n",
            "2         1                                   시간 가는 줄 모르고 읽었네요\n",
            "3         0  내내 3인칭이었다가 갑자가 1인칭 됐다가 ㅜㅜ  갑자기? 이런 순간들이 넘 많네요 ...\n",
            "4         0  소개글에 속지 마세요. 걸크러시의 크도 없는 책입니다. 전통적인 문란쓰레기남x순진녀...\n",
            "...     ...                                                ...\n",
            "97561     0                         아..............제 취향은 아닌가봐요\n",
            "97562     1                                     편한하게 웃으면서 읽었어요\n",
            "97563     0                               읽다 포기... 기대를 너무 했나 봄\n",
            "97564     1  오랜만에 몰입하며 읽었어요. 장면장면이 현실감 있게 쓰여져서 정말 드라마 한편을 보...\n",
            "97565     1  한편의   대하드라마를 보는 느낌이었어요  은봄과  진언의  삶과  사랑  감동이었...\n",
            "\n",
            "[97566 rows x 2 columns]\n",
            "\n",
            "===================\n",
            "\n",
            "초반 데이터 확인 : 97566\n",
            "중복 제거 확인 : 97566\n",
            "null값이 존재하는가?= False\n",
            "\n",
            "===================\n",
            "\n",
            "훈련용 리뷰의 개수 : 73174\n",
            "테스트용 리뷰의 개수 : 24392\n",
            "\n",
            "===================\n",
            "\n",
            "전처리 후 훈련용 샘플의 개수 : 73111\n",
            "전처리 후 테스트용 샘플의 개수 : 24371\n",
            "\n",
            "===================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9j-w6FvKRCX",
        "outputId": "f9ac8392-4fb8-4a0f-8c1c-6adf49bfc62c"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 38.5MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.7MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, beautifulsoup4, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkmq-_80J2PJ",
        "outputId": "20ea2301-43f7-486a-b147-7a9fa784ba1c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if(len(s) <= max_len):\n",
        "        cnt = cnt + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\n",
        "\n",
        "\n",
        "train_data=pd.read_csv('train.txt',encoding='utf-8')\n",
        "test_data=pd.read_csv('test.txt',encoding='utf-8')\n",
        "\n",
        "okt = Okt()\n",
        "stopwords = ['의','가','이','은','들','는','좀','원','권','과','도','를','으로','자','에','와','한','하다','뽕빨','제','금']\n",
        "\n",
        "X_train = []\n",
        "for sentence in train_data['review']:\n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "    X_train.append(temp_X)\n",
        "\n",
        "X_test = []\n",
        "for sentence in test_data['review']:\n",
        "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
        "    X_test.append(temp_X)\n",
        "\n",
        "print('\\n===================\\n')\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
        "\n",
        "vocab_size = total_cnt - rare_cnt + 1\n",
        "print('단어 집합의 크기 :',vocab_size)\n",
        "\n",
        "tokenizer = Tokenizer(vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "y_train = np.array(train_data['rate'])\n",
        "y_test = np.array(test_data['rate'])\n",
        "\n",
        "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
        "\n",
        "# 빈 샘플들을 제거\n",
        "X_train = np.delete(X_train, drop_train, axis=0)\n",
        "y_train = np.delete(y_train, drop_train, axis=0)\n",
        "print(len(X_train))\n",
        "\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
        "\n",
        "max_len = 70\n",
        "below_threshold_len(max_len, X_train)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen = max_len)\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "===================\n",
            "\n",
            "단어 집합(vocabulary)의 크기 : 28561\n",
            "등장 빈도가 2번 이하인 희귀 단어의 수: 15790\n",
            "단어 집합에서 희귀 단어의 비율: 55.28517909036799\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.5018183364057616\n",
            "단어 집합의 크기 : 12772\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "73089\n",
            "리뷰의 최대 길이 : 722\n",
            "리뷰의 평균 길이 : 17.435386993938895\n",
            "전체 샘플 중 길이가 70 이하인 샘플의 비율: 97.15825910875782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zFLZ7W8MZe3",
        "outputId": "67d38e05-a7a0-4a7c-8980-c39a8adf4926"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n",
        "\n",
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "975/975 [==============================] - 21s 13ms/step - loss: 0.3024 - acc: 0.8758 - val_loss: 0.2633 - val_acc: 0.8942\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.89417, saving model to best_model.h5\n",
            "Epoch 2/15\n",
            "975/975 [==============================] - 12s 12ms/step - loss: 0.2403 - acc: 0.9064 - val_loss: 0.2356 - val_acc: 0.9071\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.89417 to 0.90710, saving model to best_model.h5\n",
            "Epoch 3/15\n",
            "975/975 [==============================] - 12s 12ms/step - loss: 0.2178 - acc: 0.9175 - val_loss: 0.2357 - val_acc: 0.9082\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.90710 to 0.90820, saving model to best_model.h5\n",
            "Epoch 4/15\n",
            "975/975 [==============================] - 12s 12ms/step - loss: 0.2012 - acc: 0.9247 - val_loss: 0.2265 - val_acc: 0.9142\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.90820 to 0.91422, saving model to best_model.h5\n",
            "Epoch 5/15\n",
            "975/975 [==============================] - 12s 12ms/step - loss: 0.1889 - acc: 0.9307 - val_loss: 0.2350 - val_acc: 0.9092\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.91422\n",
            "Epoch 6/15\n",
            "975/975 [==============================] - 12s 12ms/step - loss: 0.1776 - acc: 0.9353 - val_loss: 0.2224 - val_acc: 0.9155\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.91422 to 0.91552, saving model to best_model.h5\n",
            "Epoch 7/15\n",
            "975/975 [==============================] - 12s 12ms/step - loss: 0.1671 - acc: 0.9403 - val_loss: 0.2360 - val_acc: 0.9138\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.91552\n",
            "Epoch 8/15\n",
            "975/975 [==============================] - 12s 12ms/step - loss: 0.1575 - acc: 0.9442 - val_loss: 0.2375 - val_acc: 0.9148\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.91552\n",
            "Epoch 9/15\n",
            "975/975 [==============================] - 12s 12ms/step - loss: 0.1476 - acc: 0.9488 - val_loss: 0.2310 - val_acc: 0.9123\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.91552\n",
            "Epoch 10/15\n",
            "975/975 [==============================] - 12s 12ms/step - loss: 0.1378 - acc: 0.9527 - val_loss: 0.2398 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.91552\n",
            "Epoch 00010: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9-v1UwxMYxm"
      },
      "source": [
        ""
      ]
    }
  ]
}